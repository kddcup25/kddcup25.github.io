<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Required meta tags always come first -->
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>CRAG-MM Challenge
</title>
  <link rel="canonical" href="./index.html">



  <link rel="stylesheet" href="./theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="./theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="./theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="./theme/css/style.css">
  <link rel="stylesheet" href="./theme/css/custom.css">


<meta name="description" content="üåü Introducing the Meta CRAG - MM: Comprehensive RAG Benchmark for Multi-modal, Multi-turn Challenge! üåü You're on vacation, strolling through ancient sites as your smart glasses share their history. Later, at a local restaurant, they translate the menu, helping you order with confidence. As the day winds down, you head back to the ‚Ä¶">
</head>

<body>
  <header class="header">
    <div class="container">
      <div class="row">
        <div class="col-xs-4 col-md-2 d-flex flex-wrap align-items-center">
          <a href="./">
            <img class="img-fluid img-logo" src=./images/logo.png alt="CRAG-MM Challenge">
          </a>
        </div>
	<div class="col-xs-12 col-md-10">
          <h1 class="title"><a href="./">CRAG-MM Challenge</a></h1>
          <ul class="list-inline">
            <li class="list-inline-item"><a href="https://arxiv.org/abs/2406.04744">CRAG Paper</a></li>
            <li class="list-inline-item"><a href="https://github.com/facebookresearch/CRAG/">Download</a></li>
            <li class="list-inline-item"><a href="https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024">KDD Cup</a></li>
            <li class="list-inline-item">|</li>
            <li class="list-inline-item"><a href="./pages/schedule.html">Workshop</a></li>
            <li class="list-inline-item"><a href="./pages/keynotes.html">Keynotes</a></li>
            <li class="list-inline-item"><a href="https://openreview.net/group?id=KDD.org/2024/Workshop/KDD_Cup_CRAG#tab-accept">Accepted Papers</a></li>
          </ul>
        </div>
      </div>
    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1>
</h1>
      <!-- <hr> -->
<article class="article">
  <div class="content">
    <!-- <h4>News</h4>

<div class="row">
<div class="alert alert-success" role="alert">
<p> [2024/09/11] The accepted papers are now available on <a href="https://openreview.net/group?id=KDD.org/2024/Workshop/KDD_Cup_CRAG">OpenReview</a>.</p>
<p> [2024/09/07] Slides of the workshop are <a href="/pages/schedule.html"> available</a>.</p>
<p> [2024/07/30] The paper submission deadline has been extended to Aug 09 2024 11:59PM UTC-0.</p>
<p> [2024/07/29] We are excited to announce <a href="https://discourse.aicrowd.com/t/meta-crag-challenge-2024-winners-announcement/">the winners of the Meta CRAG Challenge 2024</a>.</p>
<p> [2024/06/13] The CRAG paper has been featured by Hugging Face <a href="https://huggingface.co/papers?date=2024-06-10">Daily Papers</a> and covered by several media outlets (<a href="https://www.marktechpost.com/2024/06/11/advancing-reliable-question-answering-with-the-crag-benchmark/">1</a>, <a href="https://analyticsindiamag.com/comprehensive-rag-benchmark-aims-to-advance-retrieval-augmented-question-answering/">2</a>, <a href="https://adasci.org/enhancing-retrieval-augmented-generation-in-nlp-with-crag/">3</a>). </p>
</div>
</div> -->

<h2>üåü Introducing the Meta CRAG - MM: Comprehensive RAG Benchmark for Multi-modal, Multi-turn Challenge! üåü</h2>
<p>You're on vacation, strolling through ancient sites as your smart glasses share their history. Later, at a local restaurant, they translate the menu, helping you order with confidence. As the day winds down, you head back to the parking lot‚Äîno searching, no stress‚Äîyour glasses pull up an image reminder of exactly where you parked.</p>
<p>Wearable devices are revolutionising how we communicate, work, and experience the world. But to be truly valuable in everyday life, they must provide relevant, accurate, and reliable information tailored to users' needs.</p>
<h2>üí¨ Introduction</h2>
<p>Vision Large Language Models (VLLMs) have undergone significant advancements in recent years, empowering multi-modal understanding and visual question-answering (VQA) capabilities behind smart glasses. Despite the progress, VLLMs still face a major challenge: generating hallucinated answers. Studies have shown that VLLMs encounter substantial difficulties in handling queries involving long-tail entities [1]; these models also encounter challenges in handling complex queries that require the integration of different capabilities: recognition, OCR, knowledge, and generation [2].</p>
<p>The Retrieval-Augmented Generation (RAG) paradigm has expanded to accommodate multi-modal (MM) input and demonstrated promise in addressing the knowledge limitation of VLLM. Given an image and a question, an MM-RAG system constructs a search query by synthesizing information from the image and the question, searches external sources to retrieve relevant information, and then provides grounded answers to address the question [3]</p>
<p><img src="/images/MM-RAG.png"  width="600"></p>
<p><strong>Figure 1</strong>: MM-RAG</p>
<p>Despite its potential, MM-RAG still faces many challenges, such as recognizing the correct subject and comprehending the visual context in the image to understand the question, performing effective searches to retrieve useful information, synthesizing information from different sources to generate coherent and informative answers, and engaging in smooth multi-turn conversations. A comprehensive benchmark that provides a standardized framework and clear metrics is in pressing need to enable reliable and informative assessment of MM-RAG systems to facilitate and advance innovations.</p>
<hr>
<h2>üíª What is CRAG - MM: Comprehensive RAG Benchmark for multi-modal multi-turn question answering?</h2>
<p>CRAG-MM is a visual question-answering benchmark that focuses on factual questions, offering a unique collection of image and question-answering sets to enable comprehensive assessment of wearable devices. Specifically, CRAG-MM features a diverse collection of <strong>5k images</strong>, including <strong>3k egocentric ones</strong> captured by <strong>RayBan Meta smart glasses</strong>, covering <strong>13 domains</strong> and reflecting real-world challenges associated with handling egocentric images.</p>
<p>The benchmark includes <strong>4 types of questions</strong>, ranging from simple queries that can be answered by looking at the image only to complex ones that require retrieving information from multiple sources and performing reasoning.</p>
<p>Moreover, CRAG-MM encompasses both <strong>single-turn and multi-turn conversations</strong>, providing a more overarching evaluation of MM-RAG solutions.</p>
<hr>
<h2>üìÖ Timeline</h2>
<p>There will be two phases in the challenge.</p>
<p><strong>Phase 1</strong> will be open to all teams who sign up.
All teams that have at least one successful submission in Phase 1 can enter Phase 2.</p>
<h3>Phase 1: Open Competition</h3>
<ul>
<li><strong>Website Open, Sample data available and Registration Begin</strong>: March 6, 2025, 23:55 UTC</li>
<li><strong>Data Available</strong>: March 15, 2025, 23:55 UTC</li>
<li><strong>Warm-up Round Start Date</strong>: March 24, 2025, 23:55 UTC</li>
<li><strong>Phase 1 Submission Start Date</strong>: April 4, 2025, 23:55 UTC</li>
<li><strong>Phase 1 Submission End Date</strong>: May 17, 2025, 23:55 UTC</li>
</ul>
<h3>Phase 2: Competition for Top Teams</h3>
<ul>
<li><strong>Phase 2 Start Date</strong>: May 26, 2025, 23:55 UTC</li>
<li><strong>Registration and Team Freeze Deadline</strong>: June 1, 2025, 23:55 UTC</li>
<li><strong>Phase 2 End Date (Extended)</strong>: Jun 17, 2025, 23:55 UTC</li>
</ul>
<h3>Winners Announcement</h3>
<ul>
<li><strong>Winner Notification</strong>: July 1, 2025 (Tentative)</li>
<li><strong>Winner Public Announcement</strong>: August 5, 2025 (At KDD Cup Winners event)</li>
</ul>
<hr>
<h2>üèÜ Prizes</h2>
<p>The challenge boasts a prize pool of <strong>USD 33,000</strong>. There are prizes for <strong>all three tasks</strong>.</p>
<h3>Grand Prize üíé</h3>
<ul>
<li><strong>The top one team that gains the highest score for egocentric images</strong>: $5000</li>
</ul>
<h3>For Each Task</h3>
<ul>
<li>ü•á <strong>First Place</strong>: $4,000</li>
<li>ü•à <strong>Second Place</strong>: $2,500</li>
<li>ü•â <strong>Third Place</strong>: $1,500</li>
</ul>
<h3>Special Awards üíê</h3>
<ul>
<li><strong>First Place for each of the 4 question types</strong>: $1000</li>
</ul>
<p><em>The first, second, and third prize winners are not eligible to win prizes for complex question types.</em></p>
<hr>
<h2>üíª META CRAG - MM Challenge</h2>
<p>An MM-RAG QA system takes as input an image ùêº and a question ùëÑ, and outputs an answer ùê¥; the answer is generated by MM-LLMs according to information retrieved from external sources, combined with knowledge internalized in the model. A <strong>Multi-turn MM-RAG QA system</strong>, in addition, takes questions and answers from previous turns as context to answer new questions. The answer should provide useful information to answer the question without adding any hallucination.</p>
<p>We first define <strong>four types of questions</strong> in our benchmark:</p>
<ul>
<li><strong>Simple questions</strong>: Questions asking for simple facts.<ul>
<li><strong>Simple recognition</strong>: This can be directly answered from the image (e.g., "What brand is the milk?" or "Who wrote this book?" where the brand name and the book author are shown on the image).</li>
<li><strong>Simple knowledge</strong>: Requires external knowledge for the answers (e.g., "What‚Äôs the price of this sofa on Amazon?").</li>
</ul>
</li>
<li><strong>Multi-hop questions</strong>: Questions that require <strong>chaining multiple pieces of information</strong> to compose the answer (e.g., "What other movies have the director of this movie directed in the past?").</li>
<li><strong>Comparison and Aggregation questions</strong>: Questions requiring <strong>aggregating or comparing multiple pieces of information</strong> (e.g., "Which drinks do not contain added sugar among these?" or "Is this cheaper on Amazon?").</li>
<li><strong>Reasoning questions</strong>: Questions about an entity that <strong>cannot be directly looked up</strong> and require <strong>reasoning</strong> to answer (e.g., "Can the dryer be used in Europe?" where the image shows a dryer).</li>
</ul>
<p>We designed <strong>three competition tasks</strong>. As shown in Figure 2, <strong>Task #1</strong> and <strong>Task #2</strong> contain single-turn questions, where the former provides <strong>image-KG-based retrieval</strong>, and the latter additionally introduces <strong>web retrieval</strong>; Task #3 focuses on <strong>multi-turn conversations</strong>.</p>
<p>Here, we provide the content that can be leveraged in QA to ensure fair competition. We describe how we generated the data in the next section.</p>
<hr>
<h3>üèπ Challenge Tasks</h3>
<p>This challenge comprises of three tasks designed to improve multimodal question-answering (QA) systems.</p>
<p><strong>TASK #1</strong>: SINGLE-SOURCE AUGMENTATION</p>
<ul>
<li><strong>Goal</strong>: To test the <strong>basic answer generation capability</strong> of MM-RAG systems.</li>
<li>Provides an <strong>image mock API</strong> to access information from an underlying <strong>image-based mock KG</strong>. The <strong>mock KG</strong> is indexed by the image and stores structured data associated with the image. Answers to the questions <strong>may or may not exist</strong> in the mock KG. The <strong>mock API</strong> takes an image as input and returns similar images from the <strong>mock KG</strong>, along with <strong>structured data</strong> associated with each image to support answer generation.</li>
</ul>
<p><strong>TASK #2</strong>: MULTI-SOURCE AUGMENTATION</p>
<ul>
<li><strong>Goal</strong>: To test how well the <strong>MM-RAG system synthesizes information</strong> from <strong>different sources</strong>.</li>
<li>In addition to <strong>Task #1</strong>, this task provides a <strong>web search mock API</strong> as a <strong>second retrieval source</strong>. The <strong>web pages</strong> may provide <strong>useful information</strong> for answering the question but <strong>also contain noise</strong>.</li>
</ul>
<p><strong>TASK #3</strong>: MULTI-TURN QA</p>
<ul>
<li><strong>Goal</strong>: To test <strong>context understanding</strong> for <strong>smooth multi-turn conversations</strong>.</li>
<li>This task tests the system‚Äôs ability to conduct <strong>multi-turn conversations</strong>. Each conversation contains <strong>2‚Äì6 turns</strong>. Except for the first turn, <strong>questions in later turns may or may not require the image</strong> for answering.</li>
</ul>
<p>The three tasks, each building upon the previous one, <strong>guide competition teams to build end-to-end RAG systems</strong> for <strong>multi-modal, multi-turn QA</strong>.</p>
<p><img src="/images/tasks-mm.jpeg"  width="600"></p>
<p><strong>Figure 2</strong>: CRAG - MM Tasks</p>
<hr>
<h3>üíØ Evaluation Metrics</h3>
<p>We adopt exactly the <strong>same metrics and methods</strong> used in the <strong>CRAG competition</strong> to assess the performance of <strong>MM-RAG systems</strong>. Below is a brief description of the evaluation criteria.</p>
<p><strong>SINGLE-TURN QA</strong></p>
<p>For each question in the evaluation set, the answer is scored as:</p>
<ul>
<li>‚úÖ <strong>Perfect</strong>(fully correct) ‚Üí <strong>Score: 1.0</strong></li>
<li>‚ö†Ô∏è <strong>Acceptable</strong> (useful but with minor non-harmful errors) ‚Üí <strong>Score: 0.5</strong></li>
<li>‚ùì <strong>Missing</strong>: (e.g., ‚ÄúI don‚Äôt know‚Äù, ‚ÄúI‚Äôm sorry I can‚Äôt find ‚Ä¶‚Äù) ‚Üí <strong>Score: 0.0</strong></li>
<li>‚ùå <strong>Incorrect</strong>: (wrong or irrelevant answer) ‚Üí <strong>Score: -1.0</strong></li>
<li><strong>Truthfulness Score</strong>: The <strong>average score</strong> across all examples in the evaluation set for a given MM-RAG system.</li>
</ul>
<p><strong>MULTI-TURN QA</strong></p>
<p>There is not a dominant way to evaluate answer quality for multi-turn conversations. We adapt the method in [5], which is closest to the information-seeking flavor of conversations (in contrast to task fulfilling). In particular, we stop a conversation when the answers in two consecutive turns are wrong and consider answers to all remaining questions in the same conversation as missing‚Äìmimicking the behavior of real users when they lose trust or feel frustrated after repeated failures. We then take the average score of all multi-turn conversations.</p>
<hr>
<h3>üñä Evaluation Techniques</h3>
<ul>
<li><strong>Auto-evaluation</strong> is used to display scores on the <strong>leaderboard</strong>.</li>
<li>In the <strong>final round</strong>, the <strong>top-10 teams</strong> are selected via auto-evaluation.</li>
<li><strong>Manual annotations</strong> determine the <strong>final top teams</strong> for each task.</li>
</ul>
<p><strong>Performance Constraints:</strong></p>
<ul>
<li><strong>Only answer texts generated within 30 seconds are considered.</strong></li>
<li><strong>10-second timeout per turn</strong> is strictly enforced.</li>
<li><strong>Human evaluation reviews</strong> the full response for validity and hallucination.</li>
<li><strong>Automatic evaluation</strong> scores only the <strong>first 75 BPE tokens.</strong></li>
<li><strong>Full responses</strong> are checked manually for <strong>hallucination</strong>.</li>
</ul>
<hr>
<h3>üìô Evaluation Details</h3>
<ul>
<li><strong>All missing answers</strong> should return a standard response: <code>"I don't know."</code></li>
<li>Every query is associated with a <strong>query time</strong> (when the query was made).</li>
<li><strong>Dynamic questions</strong> may have different correct answers depending on query time.</li>
<li><strong>Ground truth answers</strong> are correct at the time the data was collected.</li>
</ul>
<hr>
<h2>üìä CRAG-MM Dataset Description</h2>
<p>CRAG-MM contains <strong>three parts</strong>:</p>
<ul>
<li>The Image Set</li>
<li>The QA Set</li>
<li>Retrieval Contents</li>
</ul>
<h4>üñºÔ∏è Image Set</h4>
<p>CRAG-MM contains <strong>two types of images</strong>:</p>
<ul>
<li><strong>Egocentric images</strong>: Captured using <strong>Ray-Ban Meta Smart Glasses</strong>.</li>
<li><strong>Normal images</strong>: Collected from <strong>public sources</strong>.</li>
</ul>
<h4>üìù Question Answer Pairs</h4>
<ul>
<li>Covers <strong>13 domains</strong>, including <strong>Books, Food, Math &amp; Science, Shopping, Animal, Vehicles, and more</strong>.</li>
<li>Includes <strong>4 types of questions: Simple-recognition and Simple-knowledge, Multi-hop, Comparison and Aggregation, Reasoning</strong>.</li>
<li>Contains <strong>both single-turn and multi-turn conversations</strong>.</li>
</ul>
<h4>üìÅ Retrieval Contents</h4>
<p><strong>Image Search</strong></p>
<ul>
<li>A <strong>mock image search API</strong> takes an image as input.</li>
<li>Returns <strong>similar images with structured metadata</strong> from a <strong>mock KG</strong>.</li>
<li>Example: Querying with a <strong>landmark image</strong> returns similar images <strong>with metadata</strong>.</li>
</ul>
<p><strong>Text-Based Web Search</strong></p>
<ul>
<li>A <strong>text search API</strong> takes a text query as input.</li>
<li>Returns <strong>relevant web pages</strong> (URLs, page titles, snippets, last updated time).</li>
</ul>
<p>Both APIs include <strong>hard negative data</strong> to <strong>simulate real-world challenges</strong>.</p>
<hr>
<h2>üìò Submission and Participation</h2>
<p>Participants must submit their code and model weights to run on the host's server for evaluation.</p>
<h3>üß≠ Model</h3>
<p>This KDD Cup requires participants to use Llama models to build their RAG solution. Especially, participants can use or fine-tune the following Llama 3 models from https://llama.meta.com/llama-downloads:
- Llama 3.2 11B
- Llama 3.2 90B</p>
<p>Any other non-llama models used need to be under 1.5b parameter size limit.</p>
<h3>üî® Hardware and system configuration</h3>
<p>We set a limit on the hardware available to each participant to run their solution. Specifically,</p>
<p>All submissions will be run on a single G6e instance with a NVIDIA L40s GPU with 48GB of GPU memory on AWS. Please note that:
- Llama 3.2 11B in full precision can run directly.
- Llama 3.2 90B in full precision cannot be directly run on this GPU instance. Quantization or other techniques need to be applied to make the model runnable.
- NVIDIA L40s is not using the latest architectures and hence might not be compatible with certain acceleration toolkits, so please make sure the submitted solution is compatible with the configuration.</p>
<p>Moreover, the following restrictions will also be imposed:
- The network connection will be disabled.
- Each turn in a submission has a strict 10-second timeout. If the model times out during any turn, the entire submission will be considered failed. For submissions made via the batch generation interface, the permitted time is calculated as batch size multiplied by 10 seconds per batch.
- In human evaluation, graders will assess the entire response to determine both answer validity and hallucination. In automatic evaluation, responses will be truncated to the first 75 BPE tokens for scoring purposes.
- Phase 2 submissions will be evaluated on low-resolution (960 width, 1280 height) egocentric images to mimic a real-world challenge. Resolution of normal images remains unchanged.
- To accommodate the expanded test set in Round 2, the maximum allowed evaluation time per submission has been increased from 4 hours to 7.5 hours. Submissions exceeding this limit will be terminated automatically.</p>
<h3>ü§ù Use of external resources</h3>
<p>By only providing a small development set, we encourage participants to exploit public resources to build their solutions. However, participants should ensure that the used datasets or models are publicly available and equally accessible to use by all participants. Such a constraint rules out proprietary datasets and models by large corporations. Participants are allowed to re-formulate existing datasets (e.g., adding additional data/labels manually or with Llama models), but award winners are required to make them publicly available after the competition.</p>
<h3>üîë Baseline implementation</h3>
<p>We provide users with three baseline models to help get started. Detailed implementations of these baseline models are accessible through the links provided below. Please refer to <a href="https://github.com/facebookresearch/CRAG/blob/main/docs/download_baseline_model_weights.md">this page</a> for steps to download (and check in) the models weights required for the baseline models, and refer to the inline documentation for implementation guide and further details.</p>
<ol>
<li>
<p><a href="https://github.com/facebookresearch/CRAG/blob/main/models/vanilla_llama_baseline.py"><strong>Vanilla Llama 3 Model</strong></a>.</p>
</li>
<li>
<p><a href="https://github.com/facebookresearch/CRAG/blob/main/models/rag_llama_baseline.py"><strong>RAG Baseline Model</strong></a>.</p>
</li>
<li>
<p><a href="https://github.com/facebookresearch/CRAG/blob/main/models/rag_knowledge_graph_baseline.py"><strong>RAG Knowledge Graph Baseline Model</strong></a>.</p>
</li>
</ol>
<hr>
<h2>üìò Participation and Submission</h2>
<h3>üîë Registration</h3>
<ul>
<li><strong>Teams of 1‚Äì5 participants</strong> can register on this page before submitting solutions.</li>
<li><strong>Team freeze deadline</strong>: June 1, 2025.</li>
</ul>
<h3>ü§ù Solution Submission</h3>
<ul>
<li><strong>Phase I</strong>: Each team can make <strong>6 submissions per week</strong> across <strong>all three tracks</strong>.</li>
<li><strong>Phase II</strong>: Each team can make <strong>10 total submissions</strong> across each track per week.</li>
</ul>
<h3>üíª Technical Report Submission</h3>
<p>Potential winners must submit a <strong>technical report</strong>.
- The report includes a <strong>solution description</strong> + <strong>code for reproducibility</strong>.
- <strong>Compliance with challenge rules is required for winning teams</strong>.</p>
<hr>
<h2>üèõÔ∏è KDD Cup Workshop</h2>
<p>The <strong>KDD Cup</strong> is an annual <strong>data mining and knowledge discovery competition</strong> organized by <strong>ACM SIGKDD</strong>.</p>
<p><strong>KDD Cup 2025</strong> will be held in <strong>Toronto, Canada</strong>.</p>
<p><strong>Dates</strong>: August 3‚Äì7, 2025.</p>
<hr>
<h2>‚õ∞ What Makes CRAG-MM Standout?</h2>
<ul>
<li><strong>The first publicly released wearable benchmarks</strong>: Includes <strong>real-world</strong> challenges from <strong>wearable device QA</strong>.</li>
<li><strong>Rich and insightful benchmark</strong>: Covers <strong>diverse</strong> images and questions (e.g., <strong>low-quality images, long-tail entities</strong>).</li>
<li><strong>Reliable and fair evaluation</strong>: Equal access to <strong>retrieval resources</strong> (image KG + web corpus).</li>
</ul>
<hr>
<h2>üì± Contact</h2>
<p>For inquiries, contact:
üìß <strong>crag-kddcup-2025@meta.com</strong></p>
<p>Organizers of this KDD Cup consists of scientists and engineers from <strong>Meta Reality Labs</strong> and <strong>Meta GenAI</strong>. They are:</p>
<p>‚Ä¢ Xiao Yang ‚Ä¢ Jiaqi Wang ‚Ä¢ Shervin Ghasemlou ‚Ä¢ Parth Suresh ‚Ä¢ Adam Czyzewski ‚Ä¢ Sanat Sharma ‚Ä¢ Surya Appini ‚Ä¢ Haidar Khan ‚Ä¢ Roy Luo ‚Ä¢ Ziqiang Guan ‚Ä¢ Juheon Lee ‚Ä¢ Prashan Wanigasekara ‚Ä¢ Lingkun Kong ‚Ä¢ Sajal Choudhary ‚Ä¢ Tammy Stark ‚Ä¢ Chen Zhou ‚Ä¢ Kai Sun ‚Ä¢ Shane Moon ‚Ä¢ Nicolas Scheffer ‚Ä¢ Zhaleh Feizollahi ‚Ä¢ Mangesh Pujari ‚Ä¢ Andrea Jessee ‚Ä¢ Rakesh Wanga ‚Ä¢ Rohit Patel ‚Ä¢ Anuj Kumar ‚Ä¢ Xin Luna Dong</p>
<hr>
<h2>üóÇÔ∏è Related Work</h2>
<p>To the best of our knowledge, the Meta CRAG-MM challenge is the first MM-RAG challenge for KDD Cups and broadly. CRAG-MM uniquely features natural uses cases for wearable devices based on egocentric images. Moreover, it encompasses a variety of domains and question types, effectively evaluating different capabilities of MM-RAG systems: entity recognition, OCR, query rewrite, answer generation, and so on. Furthermore, CRAG-MM extends beyond single-turn QA by including multi-turn conversations, a common and critical use case for smart assistant.</p>
<hr>
<h2>üìï References</h2>
<p>[1] Qiu et al., "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM". Available at: <a href="https://aclanthology.org/2024.findings-emnlp.14/">https://aclanthology.org/2024.findings-emnlp.14/</a></p>
<p>[2] Yu et al., "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities". Available at: <a href="https://arxiv.org/abs/2308.02490">https://arxiv.org/abs/2308.02490</a></p>
<p>[3] Gao et al., "Retrieval-Augmented Generation for Large Language Models: A Survey". Available at: <a href="https://arxiv.org/abs/2312.10997">https://arxiv.org/abs/2312.10997</a></p>
<p>[4] Yang et al., "CRAG - Comprehensive RAG Benchmark". Available at: <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/1435d2d0fca85a84d83ddcb754f58c29-Abstract-Datasets_and_Benchmarks_Track.html">https://proceedings.neurips.cc/paper_files/paper/2024/hash/1435d2d0fca85a84d83ddcb754f58c29-Abstract-Datasets_and_Benchmarks_Track.html</a></p>
<p>[5] Bai et al., "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues". Available at: <a href="https://aclanthology.org/2024.acl-long.401/">https://aclanthology.org/2024.acl-long.401/</a></p>
<!-- <hr>
<a href="/pages/organizers.html">Contributors & Acknowledgements</a> -->
  </div>
</article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
      <div class="row">
       <ul class="col-sm-6 list-inline">



        </ul>
        <p class="col-sm-6 text-sm-right text-muted">
          <a href="https://github.com/getpelican/pelican" target="_blank">powered by Pelican</a>
          /
          <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
	  /
	  <a href="https://github.com/ml4health/ml4health.github.io" target="_blank">ML4H</a>
        </p>
      </div>
    </div>
  </footer>
</body>

</html>